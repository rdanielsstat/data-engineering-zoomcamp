{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lYh7r1mTf4uo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dlt version: 1.21.0\n",
            "requests version: 2.32.5\n",
            "pandas version: 3.0.0\n",
            "json version: 2.0.9\n",
            "duckdb version: 1.4.4\n"
          ]
        }
      ],
      "source": [
        "import dlt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dlt.destinations import filesystem\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "import duckdb\n",
        "# from google.colab import userdata\n",
        "\n",
        "print(\"dlt version: \" + str(dlt.__version__))\n",
        "print(\"requests version: \" + str(requests.__version__))\n",
        "print(\"pandas version: \" + str(pd.__version__))\n",
        "print(\"json version: \" + str(json.__version__))\n",
        "print(\"duckdb version: \" + str(duckdb.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC2QnhmKxpq1"
      },
      "source": [
        "Set JSON credentials as GCP_CREDENTIALS secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsUZobVduL7l"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"DESTINATION__CREDENTIALS\"] = \"gcs.json\"\n",
        "\n",
        "# os.environ[\"BUCKET_URL\"] = \"gs://sandbox-486719-nyc-taxi-raw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPBzsEgyjsBo"
      },
      "outputs": [],
      "source": [
        "# Install for production\n",
        "# %%capture\n",
        "\n",
        "# !pip install dlt[bigquery, gs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evdUsDNbkCTk"
      },
      "outputs": [],
      "source": [
        "# Install for testing\n",
        "# %%capture\n",
        "\n",
        "# !pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0310FT-gy_P"
      },
      "source": [
        "Ingest data into DuckDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_3K97w1c2v2",
        "outputId": "4b2d26bf-2814-46fa-f80d-7a2e17417a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline rides_pipeline load step completed in 2.95 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset rides_dataset\n",
            "The duckdb destination used duckdb:////Users/rob/Projects/GitHub/data-engineering-zoomcamp/03-data-warehousing/rides_pipeline.duckdb location to store data\n",
            "Load package 1770650399.0568619 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "# Define a dlt resource to download and process Parquet files as single table\n",
        "@dlt.resource(name = \"rides\", write_disposition = \"replace\")\n",
        "\n",
        "def download_parquet():\n",
        "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata'\n",
        "\n",
        "    for month in range(1, 7):\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        yield df\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name = \"rides_pipeline\",\n",
        "    destination = \"duckdb\",  # Use DuckDB for testing\n",
        "    # destination=\"bigquery\",  # Use BigQuery for production\n",
        "    dataset_name = \"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "info = pipeline.run(download_parquet)\n",
        "\n",
        "# Print the results\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDcLjzLtooBV",
        "outputId": "74ff2de7-2f2e-41b9-a681-3dc5887f6eed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         database         schema                 name  \\\n",
            "0  rides_pipeline  rides_dataset           _dlt_loads   \n",
            "1  rides_pipeline  rides_dataset  _dlt_pipeline_state   \n",
            "2  rides_pipeline  rides_dataset         _dlt_version   \n",
            "3  rides_pipeline  rides_dataset                rides   \n",
            "\n",
            "                                        column_names  \\\n",
            "0  [load_id, schema_name, status, inserted_at, sc...   \n",
            "1  [version, engine_version, pipeline_name, state...   \n",
            "2  [version, engine_version, inserted_at, schema_...   \n",
            "3  [vendor_id, tpep_pickup_datetime, tpep_dropoff...   \n",
            "\n",
            "                                        column_types  temporary  \n",
            "0  [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...      False  \n",
            "1  [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...      False  \n",
            "2  [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...      False  \n",
            "3  [INTEGER, TIMESTAMP WITH TIME ZONE, TIMESTAMP ...      False  \n"
          ]
        }
      ],
      "source": [
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# Set search path to the dataset\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "\n",
        "# Describe the dataset to see loaded tables\n",
        "res = conn.sql(\"DESCRIBE\").df()\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVJy8JoerI2P",
        "outputId": "3f8c7fee-a9ee-4fd4-ec75-153ca60bd36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   count(1)\n",
            "0  20332093\n"
          ]
        }
      ],
      "source": [
        "# provide a resource name to query a table of that name\n",
        "with pipeline.sql_client() as client:\n",
        "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
        "        data = cursor.df()\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76zT1PzAgs7A"
      },
      "source": [
        "Ingest parquet files to GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xya0215jsnsb"
      },
      "outputs": [],
      "source": [
        "# Define a dlt source to download and process Parquet files as resources\n",
        "@dlt.source(name = \"rides\")\n",
        "\n",
        "def download_parquet():\n",
        "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
        "    for month in range(1, 7):\n",
        "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        # Return the dataframe as a dlt resource for ingestion\n",
        "        yield dlt.resource(df, name = file_name)\n",
        "\n",
        "\n",
        "with open(\"gcs.json\", \"r\") as f:\n",
        "    credentials_dict = json.load(f)\n",
        "\n",
        "my_bucket_url = \"gs://sandbox-486719-nyc-taxi-test\"\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name = \"rides_pipeline\",\n",
        "    destination = filesystem(\n",
        "        bucket_url = my_bucket_url,\n",
        "        credentials = credentials_dict,\n",
        "        layout = \"{schema_name}/{table_name}.{ext}\"\n",
        "    ),\n",
        "    dataset_name = \"rides_dataset\"\n",
        ")\n",
        "\n",
        "# Initialize the pipeline\n",
        "# pipeline = dlt.pipeline(\n",
        "#    pipeline_name = \"rides_pipeline\",\n",
        "#    destination = filesystem,\n",
        "#    dataset_name = \"rides_dataset\",\n",
        "#)\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "load_info = pipeline.run(download_parquet(), loader_file_format = \"parquet\")\n",
        "\n",
        "# Print the results\n",
        "print(load_info)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
