{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d26834ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp, unix_timestamp, max as spark_max\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d93e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs and paths\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-11.parquet\"\n",
    "folder_path = \"data/pq/yellow/2025/11\"\n",
    "output_path = folder_path  # save in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24a7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure folder exists\n",
    "os.makedirs(folder_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf0375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-11.parquet ...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "file_name = os.path.join(folder_path, \"yellow_tripdata_2025-11.parquet\")\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    print(f\"Downloading {url} ...\")\n",
    "    urllib.request.urlretrieve(url, file_name)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960b8a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/24 10:12:24 WARN Utils: Your hostname, Robs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.197 instead (on interface en0)\n",
      "26/02/24 10:12:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/24 10:12:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkProject\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e83e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4181444\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       7| 2025-11-01 00:13:25|  2025-11-01 00:13:25|              1|         1.68|         1|                 N|          43|         186|           1|       14.9|  0.0|    0.5|       1.5|         0.0|                  1.0|       22.15|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:49:07|  2025-11-01 01:01:22|              1|         2.28|         1|                 N|         142|         237|           1|       14.2|  1.0|    0.5|      4.99|         0.0|                  1.0|       24.94|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-11-01 00:07:19|  2025-11-01 00:20:41|              0|          2.7|         1|                 N|         163|         238|           1|       15.6| 4.25|    0.5|      4.27|         0.0|                  1.0|       25.62|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-11-01 00:00:00|  2025-11-01 01:01:03|              3|        12.87|         1|                 N|         138|         261|           1|       66.7|  6.0|    0.5|       0.0|        6.94|                  1.0|       86.14|                 2.5|       1.75|              0.75|\n",
      "|       1| 2025-11-01 00:18:50|  2025-11-01 00:49:32|              0|          8.4|         1|                 N|         138|          37|           2|       39.4| 7.75|    0.5|       0.0|         0.0|                  1.0|       48.65|                 0.0|       1.75|               0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file\n",
    "df = spark.read.parquet(file_name)\n",
    "print(\"Number of rows:\", df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd6196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition to 4\n",
    "df_repart = df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00393cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartitioned Parquet files written to data/pq/yellow/2025/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to parquet (overwrite if exists)\n",
    "df_repart.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Repartitioned Parquet files written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "093b89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files sizes (MB): ['24.41', '24.40', '24.43', '24.42']\n",
      "Average Parquet file size (MB): 24.42\n"
     ]
    }
   ],
   "source": [
    "# Calculate average size of parquet files\n",
    "parquet_files = [f for f in os.listdir(output_path) if f.endswith(\".parquet\")]\n",
    "sizes_mb = [os.path.getsize(os.path.join(output_path, f)) / (1024*1024) for f in parquet_files]\n",
    "\n",
    "if sizes_mb:\n",
    "    avg_size = sum(sizes_mb) / len(sizes_mb)\n",
    "    print(\"Parquet files sizes (MB):\", [\"{:.2f}\".format(s) for s in sizes_mb])\n",
    "    print(\"Average Parquet file size (MB): {:.2f}\".format(avg_size))\n",
    "else:\n",
    "    print(\"No parquet files found in\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2704e55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips on 2025-11-15: 162604\n"
     ]
    }
   ],
   "source": [
    "# Read the repartitioned folder\n",
    "df = spark.read.parquet(\"data/pq/yellow/2025/11/\")\n",
    "\n",
    "# Create a new column with just the date\n",
    "df = df.withColumn(\"pickup_date\", to_date(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Filter for November 15, 2025\n",
    "df_15 = df.filter(col(\"pickup_date\") == \"2025-11-15\")\n",
    "\n",
    "# Count trips\n",
    "trip_count = df_15.count()\n",
    "print(\"Number of trips on 2025-11-15:\", trip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f0db78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest trip in hours: 90.64666666666666\n"
     ]
    }
   ],
   "source": [
    "# Read the repartitioned Parquet folder\n",
    "df = spark.read.parquet(\"data/pq/yellow/2025/11/\")\n",
    "\n",
    "# Convert pickup and dropoff to timestamps\n",
    "df = df.withColumn(\"pickup_ts\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "       .withColumn(\"dropoff_ts\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "\n",
    "# Compute trip duration in hours\n",
    "df = df.withColumn(\"trip_hours\", (unix_timestamp(col(\"dropoff_ts\")) - unix_timestamp(col(\"pickup_ts\"))) / 3600)\n",
    "\n",
    "# Find the maximum trip duration\n",
    "max_trip_hours = df.select(spark_max(col(\"trip_hours\"))).collect()[0][0]\n",
    "print(\"Longest trip in hours:\", max_trip_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e3db44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('taxi_zone_lookup.csv', <http.client.HTTPMessage at 0x11367bbf0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the zones CSV\n",
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\"\n",
    "local_csv_path = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "urllib.request.urlretrieve(url, local_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV into a Spark DataFrame\n",
    "df_zones = spark.read.option(\"header\", \"true\").csv(local_csv_path)\n",
    "\n",
    "df_zones.write.mode(\"overwrite\").parquet(\"zones\")\n",
    "\n",
    "# Create a temp view for SQL queries\n",
    "df_zones.createOrReplaceTempView(\"zones\")\n",
    "\n",
    "# Create a temp view for the November 2025 data\n",
    "df.createOrReplaceTempView(\"yellow_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|zone_name    |trip_count|\n",
      "+-------------+----------+\n",
      "|Arden Heights|1         |\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "least_freq_zone = spark.sql(\"\"\"\n",
    "    SELECT z.Zone AS zone_name, COUNT(*) AS trip_count\n",
    "      FROM yellow_trips y\n",
    "      JOIN zones z\n",
    "        ON y.PULocationID = z.LocationID\n",
    "     GROUP BY z.Zone\n",
    "    HAVING COUNT(*) > 0\n",
    "     ORDER BY trip_count ASC\n",
    "     LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "least_freq_zone.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1185f519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Zone         |count|\n",
      "+-------------+-----+\n",
      "|Arden Heights|1    |\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Python\n",
    "# Join with zones DataFrame\n",
    "df_joined = df.join(\n",
    "    df_zones, \n",
    "    df.PULocationID == df_zones.LocationID,\n",
    "    how = 'inner'\n",
    ")\n",
    "\n",
    "# Count trips per Zone\n",
    "df_counts = df_joined.groupBy(\"Zone\").count()\n",
    "\n",
    "# Order ascending to find the least frequent pickup zone\n",
    "df_least = df_counts.orderBy(\"count\", ascending = True).limit(1)\n",
    "\n",
    "# Show the result\n",
    "df_least.show(truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07-batch-processing (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
