{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lYh7r1mTf4uo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dlt version: 1.21.0\n",
            "requests version: 2.32.5\n",
            "pandas version: 3.0.0\n",
            "json version: 2.0.9\n",
            "duckdb version: 1.4.4\n"
          ]
        }
      ],
      "source": [
        "import dlt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dlt.destinations import filesystem\n",
        "from io import BytesIO\n",
        "import json\n",
        "import os\n",
        "import duckdb\n",
        "import time\n",
        "# from google.colab import userdata\n",
        "\n",
        "print(\"dlt version: \" + str(dlt.__version__))\n",
        "print(\"requests version: \" + str(requests.__version__))\n",
        "print(\"pandas version: \" + str(pd.__version__))\n",
        "print(\"json version: \" + str(json.__version__))\n",
        "print(\"duckdb version: \" + str(duckdb.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC2QnhmKxpq1"
      },
      "source": [
        "Set JSON credentials as GCP_CREDENTIALS secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsUZobVduL7l"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"DESTINATION__CREDENTIALS\"] = \"gcs.json\"\n",
        "\n",
        "# os.environ[\"BUCKET_URL\"] = \"gs://sandbox-486719-nyc-taxi-raw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPBzsEgyjsBo"
      },
      "outputs": [],
      "source": [
        "# Install for production\n",
        "# %%capture\n",
        "\n",
        "# !pip install dlt[bigquery, gs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evdUsDNbkCTk"
      },
      "outputs": [],
      "source": [
        "# Install for testing\n",
        "# %%capture\n",
        "\n",
        "# !pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standardize_columns(df, taxi_type):\n",
        "    if taxi_type == \"green\":\n",
        "        df = df.rename(columns={\n",
        "            \"lpep_pickup_datetime\": \"pickup_datetime\",\n",
        "            \"lpep_dropoff_datetime\": \"dropoff_datetime\"\n",
        "        })\n",
        "    else:\n",
        "        df = df.rename(columns={\n",
        "            \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
        "            \"tpep_dropoff_datetime\": \"dropoff_datetime\"\n",
        "        })\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cast_types(df):\n",
        "\n",
        "    numeric_int_cols = [\n",
        "        \"VendorID\",\"passenger_count\",\"RatecodeID\",\n",
        "        \"PULocationID\",\"DOLocationID\",\"payment_type\"\n",
        "    ]\n",
        "\n",
        "    numeric_float_cols = [\n",
        "        \"trip_distance\",\"fare_amount\",\"extra\",\"mta_tax\",\n",
        "        \"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\n",
        "        \"total_amount\",\"congestion_surcharge\"\n",
        "    ]\n",
        "\n",
        "    # Convert datetimes\n",
        "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"], errors = \"coerce\")\n",
        "    df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"], errors = \"coerce\")\n",
        "\n",
        "    # Convert numeric columns safely\n",
        "    for col in numeric_int_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors = \"coerce\").astype(\"Int64\")\n",
        "\n",
        "    for col in numeric_float_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors = \"coerce\")\n",
        "\n",
        "    if \"store_and_fwd_flag\" in df.columns:\n",
        "        df[\"store_and_fwd_flag\"] = df[\"store_and_fwd_flag\"].astype(\"string\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0310FT-gy_P"
      },
      "source": [
        "Ingest data into DuckDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_3K97w1c2v2",
        "outputId": "4b2d26bf-2814-46fa-f80d-7a2e17417a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data/yellow_tripdata_2019-01.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-02.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-03.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-04.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-05.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-06.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-07.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-08.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-09.csv.gz\n",
            "Downloading data/yellow_tripdata_2019-10.csv.gz\n"
          ]
        },
        {
          "ename": "PipelineStepFailed",
          "evalue": "Pipeline execution failed at `step=extract` when processing package with `load_id=1770828488.110611` with exception:\n\n<class 'KeyboardInterrupt'>\n",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:484\u001b[39m, in \u001b[36mPipeline.extract\u001b[39m\u001b[34m(self, data, table_name, parent_table_name, write_disposition, columns, primary_key, schema, max_parallel_items, workers, table_format, schema_contract, refresh, loader_file_format)\u001b[39m\n\u001b[32m    477\u001b[39m         \u001b[38;5;28mself\u001b[39m._bump_version_and_extract_state(\n\u001b[32m    478\u001b[39m             \u001b[38;5;28mself\u001b[39m._container[StateInjectableContext].state,\n\u001b[32m    479\u001b[39m             \u001b[38;5;28mself\u001b[39m.config.restore_from_destination,\n\u001b[32m    480\u001b[39m             extract_step,\n\u001b[32m    481\u001b[39m             schema=last_schema,\n\u001b[32m    482\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m last_schema = source.schema\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:1345\u001b[39m, in \u001b[36mPipeline._extract_source\u001b[39m\u001b[34m(self, extract, source, max_parallel_items, workers, refresh, load_package_state_update)\u001b[39m\n\u001b[32m   1344\u001b[39m \u001b[38;5;66;03m# extract into pipeline schema\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1345\u001b[39m load_id = \u001b[43mextract\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_package_state_update\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_package_state_update\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[38;5;66;03m# update live schema but not update the store yet\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/extract/extract.py:487\u001b[39m, in \u001b[36mExtract.extract\u001b[39m\u001b[34m(self, source, max_parallel_items, workers, load_package_state_update)\u001b[39m\n\u001b[32m    485\u001b[39m         reset_resource_state(resource.name)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_single_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m commit_load_package_state()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/extract/extract.py:405\u001b[39m, in \u001b[36mExtract._extract_single_source\u001b[39m\u001b[34m(self, load_id, source, max_parallel_items, workers)\u001b[39m\n\u001b[32m    404\u001b[39m collector.update(\u001b[33m\"\u001b[39m\u001b[33mResources\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m, total_gens)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpipe_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpipes\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_gens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpipes\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sources\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py:158\u001b[39m, in \u001b[36mPipeIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pipe_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# if none then take element from the newest source\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     pipe_item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_source_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pipe_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# Wait for some time for futures to resolve\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/extract/pipe_iterator.py:274\u001b[39m, in \u001b[36mPipeIterator._get_source_item\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pipe_context(pipe):\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pipe_item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pipe_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# full pipe item may be returned, this is used by ForkPipe step\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;66;03m# to redirect execution of an item to another pipe\u001b[39;00m\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# else\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mdownload_taxi_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     37\u001b[39m df = standardize_columns(df, taxi_type)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m df = \u001b[43mcast_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# add metadata columns\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mcast_types\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         df[col] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoerce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstore_and_fwd_flag\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/pandas/core/tools/numeric.py:235\u001b[39m, in \u001b[36mto_numeric\u001b[39m\u001b[34m(arg, errors, downcast, dtype_backend)\u001b[39m\n\u001b[32m    234\u001b[39m     coerce_numeric = errors != \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     values, new_mask = \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_convert_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_numeric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_to_masked_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringDtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues_dtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlibmissing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNA\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# Remove unnecessary values, is expected later anyway and enables\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# downcasting\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mPipelineStepFailed\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     56\u001b[39m pipeline = dlt.pipeline(\n\u001b[32m     57\u001b[39m     pipeline_name = \u001b[33m\"\u001b[39m\u001b[33mrides_pipeline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     58\u001b[39m     destination = \u001b[33m\"\u001b[39m\u001b[33mduckdb\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Use DuckDB for testing\u001b[39;00m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# destination=\"bigquery\",  # Use BigQuery for production\u001b[39;00m\n\u001b[32m     60\u001b[39m     dataset_name = \u001b[33m\"\u001b[39m\u001b[33mrides_dataset\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m )\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Run the pipeline to load Parquet data into DuckDB\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m info = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_taxi_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(info)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:224\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    222\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:272\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    268\u001b[39m         ConfigSectionContext(\n\u001b[32m    269\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    270\u001b[39m         )\n\u001b[32m    271\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:748\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, table_format, schema_contract, refresh)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;66;03m# extract from the source\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtable_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_contract\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_contract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader_file_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_file_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28mself\u001b[39m.normalize()\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load(destination, dataset_name, credentials=credentials)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:224\u001b[39m, in \u001b[36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[32m    222\u001b[39m         trace_step = start_trace_step(trace, cast(TPipelineStep, f.\u001b[34m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     step_info = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:178\u001b[39m, in \u001b[36mwith_schemas_sync.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m._schema_storage.commit_live_schema(name)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     rv = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# because we committed live schema before calling f, we may safely\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# drop all changes in live schemas\u001b[39;00m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._schema_storage.live_schemas.keys()):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:164\u001b[39m, in \u001b[36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m should_extract_state = may_extract_state \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.restore_from_destination\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.managed_state(extract_state=should_extract_state):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:272\u001b[39m, in \u001b[36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[32m    268\u001b[39m         ConfigSectionContext(\n\u001b[32m    269\u001b[39m             pipeline_name=\u001b[38;5;28mself\u001b[39m.pipeline_name, sections=sections, merge_style=merge_func\n\u001b[32m    270\u001b[39m         )\n\u001b[32m    271\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/GitHub/data-engineering-zoomcamp/.venv/lib/python3.12/site-packages/dlt/pipeline/pipeline.py:507\u001b[39m, in \u001b[36mPipeline.extract\u001b[39m\u001b[34m(self, data, table_name, parent_table_name, write_disposition, columns, primary_key, schema, max_parallel_items, workers, table_format, schema_contract, refresh, loader_file_format)\u001b[39m\n\u001b[32m    505\u001b[39m step_info = \u001b[38;5;28mself\u001b[39m._get_step_info(extract_step)\n\u001b[32m    506\u001b[39m current_load_id = step_info.loads_ids[-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(step_info.loads_ids) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\n\u001b[32m    508\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    509\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mextract\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    510\u001b[39m     current_load_id,\n\u001b[32m    511\u001b[39m     exc,\n\u001b[32m    512\u001b[39m     step_info,\n\u001b[32m    513\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
            "\u001b[31mPipelineStepFailed\u001b[39m: Pipeline execution failed at `step=extract` when processing package with `load_id=1770828488.110611` with exception:\n\n<class 'KeyboardInterrupt'>\n"
          ]
        }
      ],
      "source": [
        "# Define a dlt resource to download and process Parquet files as single table\n",
        "@dlt.resource(name = \"rides\", write_disposition = \"replace\")\n",
        "\n",
        "def download_taxi_data():\n",
        "\n",
        "    base_urls = {\n",
        "        # \"yellow\": \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow\",\n",
        "        # \"green\": \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green\",\n",
        "        \"yellow\": \"data\",\n",
        "        \"green\": \"data\",\n",
        "    }\n",
        "\n",
        "    years = [2019, 2020]\n",
        "\n",
        "    for taxi_type, base_url in base_urls.items():\n",
        "        for year in years:\n",
        "            for month in range(1, 13):\n",
        "                \n",
        "                month_str = f\"{month:02d}\"\n",
        "                file_name = f\"{taxi_type}_tripdata_{year}-{month_str}.csv.gz\"\n",
        "                url = f\"{base_url}/{file_name}\"\n",
        "\n",
        "                print(f\"Downloading {url}\")\n",
        "\n",
        "                # retry logic (network hiccups happen)\n",
        "                max_retries = 3\n",
        "\n",
        "                for attempt in range(max_retries):\n",
        "                    try:\n",
        "                        df = pd.read_csv(\n",
        "                            url,\n",
        "                            compression = \"gzip\",\n",
        "                            low_memory = False,   # avoid dtype guessing warning\n",
        "                            dtype = str           # force schema stability across months\n",
        "                        )\n",
        "\n",
        "                        df = standardize_columns(df, taxi_type)\n",
        "                        df = cast_types(df)\n",
        "\n",
        "                        # add metadata columns\n",
        "                        df[\"taxi_type\"] = taxi_type\n",
        "                        df[\"year\"] = str(year)\n",
        "                        df[\"month\"] = month_str\n",
        "\n",
        "                        yield df\n",
        "                        break  # success, exit retry loop\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed attempt {attempt+1} for {file_name}: {e}\")\n",
        "                        time.sleep(2)\n",
        "\n",
        "                        if attempt == max_retries - 1:\n",
        "                            print(f\"Skipping file after retries: {file_name}\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name = \"rides_pipeline\",\n",
        "    destination = \"duckdb\",  # Use DuckDB for testing\n",
        "    # destination=\"bigquery\",  # Use BigQuery for production\n",
        "    dataset_name = \"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "info = pipeline.run(download_taxi_data)\n",
        "\n",
        "# Print the results\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDcLjzLtooBV",
        "outputId": "74ff2de7-2f2e-41b9-a681-3dc5887f6eed"
      },
      "outputs": [],
      "source": [
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# Set search path to the dataset\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "\n",
        "# Describe the dataset to see loaded tables\n",
        "res = conn.sql(\"DESCRIBE\").df()\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVJy8JoerI2P",
        "outputId": "3f8c7fee-a9ee-4fd4-ec75-153ca60bd36f"
      },
      "outputs": [],
      "source": [
        "# provide a resource name to query a table of that name\n",
        "with pipeline.sql_client() as client:\n",
        "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
        "        data = cursor.df()\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76zT1PzAgs7A"
      },
      "source": [
        "Ingest parquet files to GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xya0215jsnsb"
      },
      "outputs": [],
      "source": [
        "# Define a dlt source to download and process Parquet files as resources\n",
        "@dlt.source(name = \"rides\")\n",
        "\n",
        "def download_parquet():\n",
        "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
        "    for month in range(1, 7):\n",
        "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        # Return the dataframe as a dlt resource for ingestion\n",
        "        yield dlt.resource(df, name = file_name)\n",
        "\n",
        "\n",
        "with open(\"gcs.json\", \"r\") as f:\n",
        "    credentials_dict = json.load(f)\n",
        "\n",
        "my_bucket_url = \"gs://sandbox-486719-nyc-taxi-test\"\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name = \"rides_pipeline\",\n",
        "    destination = filesystem(\n",
        "        bucket_url = my_bucket_url,\n",
        "        credentials = credentials_dict,\n",
        "        layout = \"{schema_name}/{table_name}.{ext}\"\n",
        "    ),\n",
        "    dataset_name = \"rides_dataset\"\n",
        ")\n",
        "\n",
        "# Initialize the pipeline\n",
        "# pipeline = dlt.pipeline(\n",
        "#    pipeline_name = \"rides_pipeline\",\n",
        "#    destination = filesystem,\n",
        "#    dataset_name = \"rides_dataset\",\n",
        "#)\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "load_info = pipeline.run(download_parquet(), loader_file_format = \"parquet\")\n",
        "\n",
        "# Print the results\n",
        "print(load_info)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
